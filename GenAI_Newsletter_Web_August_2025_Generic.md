# New Releases and Blogs
## Bedrock
### New Models
* OpenAI's Open Weight Models*: now available on Bedrock and SageMaker JumpStart, with support for batch inference in Bedrock. These models excel in STEM, and have advanced reasoning capabilities for agentic workflows, coding and mathematical problem-solving. Running in Bedrock, these models are 10x more price-performant than the comparable Gemini model, 18x more than DeepSeek-R1, and 7x more than the comparable OpenAI o4 model. [Learn More](https://www.aboutamazon.com/news/aws/openai-models-amazon-bedrock-sagemaker)
* Anthropic Claude Opus 4.1*: now available in Bedrock, Claude Opus 4.1 is Anthropic's most intelligent model to date and an industry leader for coding and agents. Anthropic believes Opus 4.1 shines in agentic search and research, content creation, memory and context management.[Learn More](https://aws.amazon.com/about-aws/whats-new/2025/08/anthropic-claude-opus-4-1-amazon-bedrock/)
* Mistral Small 3.2 (24B) Instruct-2506*: now available on Bedrock Marketplace and SageMaker JumpStart. The model is optimized for enhanced instruction following and reduced repetition errors. It is well-suited for enterprise applications where reliability and precision are critical. With a 128,000-token context window, the model can process extensive documents and maintain context throughout longer conversations.[Learn More](https://aws.amazon.com/blogs/machine-learning/mistral-small-3-2-24b-instruct-2506-is-now-available-on-amazon-bedrock-marketplace-and-amazon-sagemaker-jumpstart/)
### Bedrock AgentCore (new blogs)
* AgentCore Browser Tool*: a fully managed, pre-built cloud-based browser tool enables AI agents to interact seamlessly with websites, creating new possibilities for educational applications. EdTech companies can build agents that assist students with research by navigating educational resources, pulling relevant information from multiple sources, and synthesizing findings into coherent study materials. [Learn More](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-browser-tool/)
* AgentCore Code Interpreter*: Bedrock AgentCore Code Interpreter is a fully managed service that enables AI agents to securely execute code in isolated sandbox environments. This solves challenges around security, scalability, and infrastructure management when deploying AI agents that need computational capabilities.[Learn More](https://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-bedrock-agentcore-code-interpreter/)
* AgentCore Gateway*: Bedrock AgentCore Gateway transforms enterprise AI agent tool development by providing a unified interface where AI agents can discover, access, and invoke tools. EdTech organizations can use this service to develop and manage a suite of educational tools that work together seamlessly, from content creation assistants to student support chatbots.[Learn More](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-gateway-transforming-enterprise-ai-agent-tool-development/)
* AgentCore Identity*: Bedrock AgentCore Identity helps secure agentic AI at scale by providing robust identity management capabilities. EdTech companies can implement role-based access controls that ensure appropriate content access based on user roles such as students, teachers, or administrators, while maintaining compliance with educational privacy regulations.[Learn More](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-identity-securing-agentic-ai-at-scale/)
### Other
* Inline Code Nodes in Bedrock Flows*: Bedrock Flows now supports inline code nodes in public preview, enabling you to write Python scripts directly within your workflow, alleviating the need for separate AWS Lambda functions. EdTech companies can leverage this feature to create complex, conditional learning pathways that respond dynamically to student inputs and performance metrics. The inline code capability allows for advanced data transformations, custom scoring algorithms, and personalized feedback mechanisms within a visual interface.[Learn More](https://aws.amazon.com/blogs/machine-learning/inline-code-nodes-now-supported-in-amazon-bedrock-flows-in-public-preview/)
* Intelligent Document Processing with Gen AI*: AWS has introduced an open source ready-to-deploy solution - GenAI IDP Accelerator for intelligent document processing using Bedrock. EdTech companies can use this solution to automatically extract and analyze information from educational materials, student submissions, and administrative documents at scale.[Learn More](https://aws.amazon.com/blogs/machine-learning/accelerate-intelligent-document-processing-with-generative-ai-on-aws/)
* Automated Reasoning Checks in Bedrock Guardrails Now GA*: Automated Reasoning checks help you validate the accuracy of content generated by FMs against a domain knowledge, minimizing AI hallucinations with up to 99% verification accuracy. EdTechs can ensure that AI-generated content remains factually correct. [Learn More](https://aws.amazon.com/blogs/aws/minimize-ai-hallucinations-and-deliver-up-to-99-verification-accuracy-with-automated-reasoning-checks-now-available/)
## SageMaker AI
* AWS Batch Support for SageMaker Training Jobs*: SageMaker now supports AWS Batch for managing compute resources during model training, allowing greater flexibility and cost optimization for large-scale machine learning workloads. This allows you to automate queuing, scheduling, and retries so you can maximize GPU utilization while cutting costs. EdTech companies can use this feature to efficiently train and fine-tune large language models on educational content and student data at scale. [Learn More](https://aws.amazon.com/blogs/machine-learning/introducing-aws-batch-support-for-amazon-sagemaker-training-jobs/)
* Fine-tune OpenAI GPT-OSS models*: Learn how to fine-tune a GPT-OSS model in a fully managed training environment using SageMaker AI training jobs and Hugging Face libraries. [Learn More](https://aws.amazon.com/blogs/machine-learning/fine-tune-openai-gpt-oss-models-on-amazon-sagemaker-ai-using-hugging-face-libraries/)
## Amazon Q 
* Q Developer CLI Custom Agents*: Q Developer CLI now supports custom agents that you can tailor for specialized tasks such as code reviews and troubleshooting. These customizable agents allow developers to create task-specific assistants that enhance productivity for particular workflows or codebases. EdTech development teams can leverage these custom agents to accelerate feature development through specialized code review agents that understand educational software requirements or create agents that help troubleshoot common issues in learning platform infrastructures.[Learn More](https://aws.amazon.com/blogs/devops/overcome-development-disarray-with-amazon-q-developer-cli-custom-agents/)
## Other
* Building an AI-Driven Course Content Generation System*: AWS has released a comprehensive guide on building AI-driven course content generation systems using Bedrock, and serverless services such as SQS, Lambda and API Gateway enabling automated creation of educational materials at scale. The solution leverages foundation models to transform course outlines into complete content sets including lectures, assessments, and supplementary materials while maintaining pedagogical quality and consistency.[Learn More](https://aws.amazon.com/blogs/machine-learning/building-an-ai-driven-course-content-generation-system-using-amazon-bedrock/)
* Strands Agents SDK*: AWS has released a technical deep dive on the Strands Agents SDK, an open-source toolkit for building, managing, and observing AI agents with advanced architectures including context management and memory capabilities. The observability features allow educational developers to understand agent reasoning, troubleshoot issues in student-agent interactions, and continuously improve the educational effectiveness of AI tutors based on detailed performance metrics and conversation traces.[Learn More](https://aws.amazon.com/blogs/machine-learning/strands-agents-sdk-a-technical-deep-dive-into-agent-architectures-and-observability/)

# Customer Success Stories
* Empowering Students with Disabilities*: University Startups developed a GenAI assistant on Bedrock, which creates personalized transition plans for high school students with disabilities to explore further education and careers after graduation. [Learn More](https://aws.amazon.com/blogs/machine-learning/empowering-students-with-disabilities-university-startups-generative-ai-solution-for-personalized-student-pathways/)
* University of Texas Austin Personalized Learning Support via a Tutor Platform Powered by 9 AWS Services*: UT Austin collaborated with AWS GenAIIC team to develop UT Sage, a faculty-guided, Gen AI tutor platform. UT Sage provides conversational course-related support on-demand while aligning with responsible AI frameworks and preserving the essential connection between faculty and students.
* Infosys Topaz Transforms Technical Help Desk Operations*: Infosys Topaz developed a Gen AI application using Bedrock and OpenSearch to optimize technical help desk operations, reducing the percentage of issues requiring human intervention from 30â€“40% to 20%. The platform uses Claude and Titan models to analyze user queries, troubleshoot issues, and provide contextualized assistance based on enterprise-specific knowledge. [Learn More](https://aws.amazon.com/blogs/machine-learning/how-infosys-topaz-leverages-amazon-bedrock-to-transform-technical-help-desk-operations/)

# Emerging Trends in GenAI
## Agentic LLMs
The AI industry is witnessing a fundamental shift as models evolve to incorporate native agentic capabilities directly within their architectures, rather than relying on external orchestration systems. [Anthropic's Claude 3.7 Sonnet](https://aws.amazon.com/blogs/aws/anthropics-claude-3-7-sonnet-the-first-hybrid-reasoning-model-is-now-available-in-amazon-bedrock/) pioneered the "hybrid reasoning" approach, allowing a single model to toggle between rapid responses and extended thinking modes, integrating reasoning as a core capability rather than requiring separate models. This architectural innovation has been adopted by other leading models, with [GLM-4.5](https://z.ai/blog/glm-4.5) introducing hybrid thinking modes that can "alternate between a 'thinking' mode for sophisticated reasoning and tool employment, and a 'non-thinking' mode for fast, direct answers," while achieving a 90.6% tool calling success rate. Most recently, [DeepSeek's V3.1](https://api-docs.deepseek.com/news/news250821) has exemplified this trend by featuring "stronger agent skills" through post-training that boosts tool use and multi-step agent tasks, positioning itself as "our first step toward the agent era". This evolution signaling that agentic behavior is becoming an intrinsic model capability rather than an external application layer.
## MCP ecosystem is accelerating
The Model Context Protocol (MCP) ecosystem is experiencing rapid acceleration, with major companies and platforms launching official MCP servers to integrate their services with AI agents. The ecosystem's maturation is evidenced by [Cloudflare's collaboration with 10 leading companies](https://blog.cloudflare.com/mcp-demo-day/) including Anthropic, Atlassian, Block, and Stripe to launch remote MCP servers, moving beyond local installations to make MCP servers accessible "the same way you would a website: type a URL and go". 

[AWS has released MCP servers](https://awslabs.github.io/mcp/) for Lambda, ECS, EKS, and many other services since April 2025, enabling AI assistants to incorporate AWS best practices directly into development workflows. AWS continues to expand its MCP offerings with specialized tools like the [Amazon DynamoDB data modeling server](https://aws.amazon.com/blogs/database/introducing-the-amazon-dynamodb-data-modeling-mcp-tool/), which can reduce data modeling time "from days or even weeks of research and iteration" to hours by providing "structured, natural-language-driven workflow to translate application requirements into DynamoDB data models". 

[GitHub's recent open-sourcing of its MCP server](https://github.blog/open-source/maintainers/why-we-open-sourced-our-mcp-server-and-what-it-means-for-you/?utm_source=alphasignal&utm_campaign=2025-08-25&asuniq=ed5e9bb0#how-to-get-started-using-the-github-remote-mcp-server) marks a significant milestone, providing a "source-of-truth interface between GitHub and any LLM, reducing hallucinations and unlocking new automation workflows", enabling developers to ask natural language questions like "What's the status of PR #72?" and receive real-time data rather than hallucinated responses. 

This surge in official MCP server releases from established platforms indicates the protocol's transition from experimental technology to enterprise-ready infrastructure for AI agent connectivity.
